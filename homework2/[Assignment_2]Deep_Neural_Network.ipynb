{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your Deep Neural Network: Step by Step\n",
    "\n",
    "첫 실습에서 구현했던 logistic regression을 확장해서 많은 계층을 갖는 (deep) neural network를 구현해봅니다. \n",
    "\n",
    "\n",
    "- 본 실습에서는, deep neural network를 구현하는데 필요한 기능들을 구현합니다.\n",
    "- 본 실습에서 구현된 deep neural network는 다음 실습에서 고양이 사진을 구별하는데 사용됩니다. \n",
    "\n",
    "\n",
    "**본 실습을 완성하면 다음에 대해서 이해하게 됩니다.**\n",
    "- ReLU같은 non-linear activation unit을 사용하여 모델을 개선합니다.\n",
    "- 여러개의 hidden layers를 갖는 깊은 신경망을 구현합니다.\n",
    "- neural network를 class로 구현하여 사용을 쉽게 합니다.\n",
    "\n",
    "**Notation**:\n",
    "- Superscript $[l]$ 는 $l$번째 계층을 나타내기 위해서 사용합니다. \n",
    "    - Example: $a^{[l]}$ 는 $l^{th}$ 계층의 activation입니다. $W^{[l]}$와 $b^{[l]}$는 $l$번째 계층의 파라미터(weights)입니다. \n",
    "- Superscript $(i)$는 $i^{th}$ example 입니다. \n",
    "    - Example: $x^{(i)}$는 $i^{th}$ training example입니다.\n",
    "- Lowerscript $i$는 벡터에서 $i$번째 엔트리입니다.\n",
    "    - Example: $a^{[l]}_i$는 $l^{th}$ 계층의 activations에서 $i$번째 엔트리입니다..\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - 시작하기전에...\n",
    "## 저 이제욱은 본 과제를 수행하면서 다른사람의 도움을 받거나 주지 않았습니다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages\n",
    "\n",
    "본 과제에는 다음의 패키지들이 필요합니다. \n",
    "- [numpy](www.numpy.org)는 Python에서 과학연산을 위한 기본 패키지입니다.\n",
    "- [matplotlib](http://matplotlib.org)는 파이선에서 그래프를 그리기 위한 패키지입니다.\n",
    "- dnn_utils 에는 `relu()`, `relu_backward()` 함수처럼 본과제에서 필요한 몇가지 유용한 함수를 제공합니다.\n",
    "- testCases 는 구현이 잘되었는지 체크하는 함수들이 있습니다.\n",
    "- np.random.seed(1)는 모든 실험이 동일한 결과를 얻을수있도록합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v4a import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Outline of the Assignment\n",
    "\n",
    "본 과제에서는 뉴럴 네트워크에서 필요한 여러가지 'helper'함수들을 구현합니다. 이러한 함수들은 다음 과제에서 2개 이상의 계층을 갖는 네트워크를 만들때 사용됩니다. \n",
    "각 함수를 구현할때 필요한 설명이 주어져 있으니, 먼저 잘 읽어보도록 하세요. \n",
    "다음과 같은 함수들을 구현할 것입니다.\n",
    "\n",
    "1) $L$계층을 갖는 뉴럴네트워크의 파라미터를 초기화한다.\n",
    "2) Forward propagation의 전체과정을 다음의 순서로 구현합니다. \n",
    "    - 먼저, $Z^{[\\ell]}$을 출력하는 LINEAR 파트를 구현합니다. \n",
    "    - Non-linear ACTIVATIOPN 함수인 relu와 sigmoid의 구현은 dnn_utils_v2.py에 이미 구현되어있습니다. \n",
    "    - LINEAR한 부분과 ACTIVATION을 결합하여, [LINEAR->ACTIVATION]을 처리하는 함수를 구현합니다. \n",
    "    - [LINEAR->RELU]를 $L-1$번 반복하여 결합하고, [LINEAR->SIGMOID]를 마지막에 결합한 L_model_forward 함수를 구현합니다. 이 함수는 forward propatation의 전 과정을 수행합니다.\n",
    "\n",
    "3) loss (또는 cost)를 구하는 함수를 구현합니다. \n",
    "\n",
    "4) Backward propagation을 다음 순서로 구현합니다. \n",
    "    - backward propagation의 LINEAR한 파트를 구현합니다.\n",
    "    - Non-linear ACTIVATION 함수의 backward propagation은 relu_backward/sigmoid_backward에 이미 구현되어 있습니다. \n",
    "    - 앞의 두 과정을 결합하여, [LINEAR->ACTIVATION]에 대한 backward propagation을 수행하는 함수를 구현합니다. \n",
    "    - [LINEAR->RELU]를 $L-1$번 반복하여 결합하고, [LINEAR->SIGMOID]를 마지막에 결합한 L_model_backward 함수를 구현합니다. 이 함수는 backward propatation의 전 과정을 수행합니다.\n",
    "\n",
    "\n",
    "<img src=\"images/final outline.png\" style=\"width:800px;height:500px;\">\n",
    "\n",
    "\n",
    "**Note:** 모든 forward함수는 이에 대응하는 backward함수가 필요합니다. 따라서, forward함수에서는 backward함수에서 gradients를 계산할때 필요한 값들을 cache에 저장하여 사용합니다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Initialization\n",
    "\n",
    "$L$ 계층을 갖는 네트워크를 초기화 합니다. 네트워크의 모든 파라미터들($W^{[l]}$과 $b^{[l]}$)을 초기화 합니다. \n",
    "\n",
    "**Instructions**:\n",
    "- 텐서를 랜덤값으로 초기화 하기 위해서는 `np.random.randn(shape)*0.01`을 사용합니다. 0으로 초기화하지 않는 이유에 대해서 생각해보세요.\n",
    "- bias는 0으로 초기화 합니다. `np.zeros(shape)`를 사용합니다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - L-layer Neural Network\n",
    "\n",
    "\n",
    "$L$계층을 갖는 네트워크에서 $\\ell$번째 계층은 $n^{[\\ell]}$개의 유닛(뉴런)을 갖습니다. 따라서, 입력이 $m=209$개의 examples를 갖는 경우 입력 $X$의 모양은 $(12288, 209)$이며, 각 계층의 파라미터와 activation의 모양은 다음과 같습니다. \n",
    "\n",
    "|               | Shape of **W**   |  Shape of **b** |  **Activation**  | **Shape of Activation**   |\n",
    "|---------------|------------------|-----------------|------------------|---------------------------|\n",
    "|**Layer 1**    | $(n^{[1]},12288)$| $(n^{[1]},1)$   |  $A^{[1]} = \\sigma(W^{[1]}  X + b^{[1]})$ |  $(n^{[1]},209)$   |\n",
    "|**Layer 2**    | $(n^{[2]},n^{[1]})$| $(n^{[2]},1)$   |  $A^{[2]} = \\sigma(W^{[2]}  X + b^{[2]})$ |  $(n^{[2]},209)$   |\n",
    "| ...           | ...              |                 |                  |                            |\n",
    "|**Layer L-1**    | $(n^{[L-1]},n^{[L-2]})$| $(n^{[L-1]},1)$   |  $A^{[L-1]} = \\sigma(W^{[L-1]}  X + b^{[L-1]})$ |  $(n^{[L-1]},209)$   |\n",
    "|**Layer L**    | $(n^{[L]},n^{[L-1]})$| $(n^{[L]},1)$   |  $A^{[L]} = \\sigma(W^{[L]}  X + b^{[L]})$ |  $(n^{[L]},209)$   |\n",
    "\n",
    "\n",
    "파이선에서 $W X + b$를 계산할때 broadcating을 통해 모양을 맞추는것을 기억하세요. \n",
    "예를 들어서...\n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{2}$$\n",
    "\n",
    "$WX + b$를 계산하면 아래와 같습니다. $b$가 column의 개수 만큼 broadcating되어서 각 column에 더해집니다.\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{3}  $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: L-layer Neural Network의 파라미터를 초기화합니다.\n",
    "\n",
    "**Instructions**:\n",
    "- 모델은 다음의 구조를 갖습니다. *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*. 즉, ReLU를 activation으로 갖는 $L-1$개의 계층과 sigmoid를 activation으로 갖는 마지막 계층으로 구성됩니다. \n",
    "- W는 랜덤으로 초기화 합니다. `np.random.randn(shape) * 0.01`를 사용하세요.\n",
    "- bias는 0으로 초기화합니다. `np.zeros(shape)`를 사용하세요. \n",
    "- 함수에 인자로 주어지는 `layer_dims`는 각 계층의 뉴런의 개수 $n^{[l]}$를 갖는 리스트입니다. 예를 들어 [2,4,1]는 2개의 입력, 첫번째 hidden layer에는 4개의 뉴런, 마지막 출력은 1개인 네트워크를 나타냅니다. 이에 따라서, $W^{[1]}$의 모양은 (4,2), $W^{[2]}$의 모양은 (1,4), $b^{[1]}$의 모양은 (4,1), $b^{[2]}$의 모양은 (1,1)입니다. $L$계층의 네트워크에서는 이를 일반화시켜서 생각합니다.\n",
    "- 만약에 $L=1$인 1계층만 갖는 네트워크라면, 다음처럼 초기화 할것입니다. $L$이 1보다 큰 일반적인 경우에 대해서 구현하세요. \n",
    "```python\n",
    "    if L == 1:\n",
    "        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)  # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> W1 </td>\n",
    "    <td>[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
    " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
    " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
    " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>b1 </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>W2 </td>\n",
    "    <td>[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
    " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
    " [-0.00768836 -0.00230031  0.00745056  0.01976111]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>b2 </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Forward propagation module\n",
    "\n",
    "### 4.1 - Linear Forward \n",
    "네트워크의 파라미터를 초기화 했으니, 이제는 forward propagation함수를 구현합니다. \n",
    "다음의 함수를 단계적으로 구현합니다. \n",
    "- LINEAR\n",
    "- LINEAR -> ACTIVATION : 여기서 ACTIVATION은 ReLU 또는 Sigmoid입니다.\n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID :이것이 전체 모델입니다.\n",
    "\n",
    "LINEAR 부분은 다음의 연산을 수행합니다. \n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "\n",
    "여기서 $A^{[0]} = X$입니다. $A^{[0]}$는 모든 examples를 가지고 있는 매트릭스(rank-2 텐서)입니다.\n",
    "\n",
    "**Exercise**: forward propagation의 LINEAR파트를 구현합니다. \n",
    "\n",
    "**Reminder**:\n",
    "$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$를 계산하기 위해서 `np.dot()`을 사용하세요. 텐서의 차원이 맞는지 확인하기 위해서는 `print(W.shape)`처럼 모양을 찍어서 디버깅합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W,A)+b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td> **Z** </td>\n",
    "    <td> [[ 3.26295337 -1.23429987]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Linear-Activation Forward\n",
    "\n",
    "이번 과제에서는 2가지의 activation 함수가 사용됩니다. \n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. \n",
    "`sigmoid()`함수는 이미 구현되어 있습니다. 이 함수는 2개의 값을 반환합니다. \"`A`\"와 \"`cache`\"입니다. `cache`에는 \"`Z`\" 값을 갖습니다. `sigmoid`함수는 다음과 같이 사용합니다. \n",
    "``` python\n",
    "A, activation_cache = sigmoid(Z)\n",
    "```\n",
    "\n",
    "- **ReLU**: RELU(Z) = max(0, Z)$. `relu()`함수도 이미 구현되어 있습니다. 이 함수는 2개의 값을 반환합니다. \"`A`\"와 \"`cache`\"입니다. `cache`에는 \"`Z`\" 값을 갖습니다. `relu`함수는 다음과 같이 사용합니다. \n",
    "``` python\n",
    "A, activation_cache = relu(Z)\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINEAR과 ACTIVATION을 두개를 합쳐서 하나의 함수로 사용하는 것이 편리합니다. \n",
    "LINEAR를 먼저 호출하고, 그 결과를 ACTIVATION에 적용합니다. \n",
    "\n",
    "**Exercise**: 이 함수는 *LINEAR->ACTIVATION*를 수행합니다. 이를 수식으로 표현하면 다음과 같습니다: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$. 여기서 \"$g$\"는 sigmoid()또는 relu() 입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "            stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, activation_cache = sigmoid(np.dot(W,A_prev)+b)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, activation_cache = relu(np.dot(W,A_prev)+b)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "       \n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td> With sigmoid: A </td>\n",
    "    <td > [[ 0.96890023  0.11013289]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> With ReLU: A </td>\n",
    "    <td > [[ 3.43896131  0.        ]]</td> \n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: 딥러닝에서는 \"[LINEAR->ACTIVATION]\"를 1개의 계층(layer)로 카운트합니다. 2개 계층이 아닙니다. 파라미터가 없는 연산은 계층으로 카운트하지 않는것이 일반적입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) L-Layer Model \n",
    "\n",
    "$L$계층의 뉴럴 네트워크에서는 ReLU를 사용하는 `linear_activation_forward`를 $L-1$번 반복하고, 마지막에는 SIGMOID를 사용하는 `linear_activation_forward`를 호출하는 하나의 모델로 만들게 됩니다. \n",
    "\n",
    "<img src=\"images/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "**Exercise**: 위의 모델을 함수로 구현합니다. \n",
    "\n",
    "**Instruction**: 아래 코드에서 `AL`은 $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$입니다. 즉, `AL`은 최종 예측값인 $\\hat{Y}$입니다. \n",
    "\n",
    "**Tips**:\n",
    "- 앞에서 구현한 함수들을 사용해서 구현합니다. \n",
    "- 루프에서는 [LINEAR->RELU]를 (L-1)번 호출합니다.\n",
    "- forward에서는 cache를 계속해서 리스트에 추가해야합니다. `caches`라는 리스트에 `c`를 추가하기 위해서는 `caches.append(c)`라고 하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A,cache = linear_activation_forward(A_prev,parameters['W' + str(l)],parameters['b' + str(l)],'relu')\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    A_prev = A \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL,cache = linear_activation_forward(A_prev,parameters['W' + str(L)],parameters['b' + str(L)],'sigmoid')\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "# print(np.dot(parameters['W1'],X)+parameters['b1'])\n",
    "# print(linear_activation_forward(X,parameters['W1'],parameters['b1'],relu))\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <td> AL </td>\n",
    "    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> Length of caches list </td>\n",
    "    <td > 3 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 forward propagation함수가 모두 완성되었습니다. 입력 X가 주어지면 마지막 출력인 $A^{[L]}$는 `m`개의 입력에 대한 예측값을 갖고 있습니다. \n",
    "각 계층에서의 중간 계산값들은 모두 \"`caches`\"에 저장되어 backward propagation에 사용되게 됩니다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Cost function\n",
    "\n",
    "현재 모델이 얼마나 잘하고 있는지를 나타내는 cost함수를 계산합니다. \n",
    "\n",
    "**Exercise**: cost함수 $J$를 계산하기 위해서는 다음 식으로 주어진 cross-entropy 함수를 사용합니다: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\{y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)\\} \\tag{7}$$ \n",
    "이 함수는 미리 구현을 해두었습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = -1./m * np.sum((Y * np.log(AL) + (1-Y) * np.log(1 - AL)), axis=1, keepdims=True)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.2797765635793422\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "    <tr>\n",
    "    <td>cost</td>\n",
    "    <td> 0.2797765635793422</td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Backward propagation module\n",
    "\n",
    "forward propagation과 마찬가지로, backward propagation을 위한 함수들을 순차적으로 구현합니다. \n",
    "backward propagation에서는 파라미터에 대한 gradient를 구합니다. \n",
    "\n",
    "\n",
    "<img src=\"images/backprop_kiank.png\" style=\"width:650px;height:250px;\">\n",
    "\n",
    "backward propagation에서는 다음의 3단계로 진행됩니다. \n",
    "- LINEAR backward\n",
    "- LINEAR -> ACTIVATION backward: 여기서 ACTIVATION은 ReLU 또는 sigmoid입니다.\n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward (모델 전체)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 - Linear backward\n",
    "\n",
    "계층 $l$에서, LINEAR파트는 $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$에 해당합니다. \n",
    "\n",
    "$dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$입니다. $dZ^{[l]}$는 이미 계산되었고, backward propagation되어 전달된 상태입니다. 전달받은 $dZ^{[l]}$를 이용해서 $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$를 구합니다.\n",
    "\n",
    "<img src=\"images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
    "<caption>Figure 4</caption>\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: 위의 3개 식을 이용하여 `linear_backward()`함수를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    # Here cache is \"linear_cache\" containing (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = np.dot(dZ,A_prev.T)/m\n",
    "    db = np.sum(dZ,axis=1,keepdims=True)/m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
      " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
      " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
      " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
      " [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
      "dW = [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
      " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
      " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n",
      "db = [[-0.14713786]\n",
      " [-0.11313155]\n",
      " [-0.13209101]]\n"
     ]
    }
   ],
   "source": [
    "# Set up some test inputs\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "    \n",
    "```\n",
    "dA_prev = \n",
    " [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
    " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
    " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
    " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
    " [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
    "dW = \n",
    " [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
    " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
    " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n",
    "db = \n",
    " [[-0.14713786]\n",
    " [-0.11313155]\n",
    " [-0.13209101]]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Linear-Activation backward\n",
    "\n",
    " 위에서 구현한 `linear_backward` 함수를 이용해서 `linear_activation_backward`를 구현합니다. \n",
    "\n",
    " $g(.)$가 activation함수라면, `dZ`는 다음과 같이 계산됩니다:\n",
    "$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$.  \n",
    "\n",
    " `linear_activation_backward`구현에 필요한 activation에 대한 backward함수는 다음처럼 이미 구현되어 제공됩니다. \n",
    " \n",
    "- `sigmoid_backward`: SIGMOID 에 대한 backward함수는 다음처럼 사용합니다:\n",
    "\n",
    "```python\n",
    "dZ = sigmoid_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "- **`relu_backward`**: RELU에 대한 backward함수는 다음처럼 사용합니다:\n",
    "\n",
    "```python\n",
    "dZ = relu_backward(dA, activation_cache)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    dA_prev, dW, db= linear_backward(dZ,linear_cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989  0.        ]\n",
      " [ 0.37883606  0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output with sigmoid:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td>\n",
    "        dA_prev\n",
    "     </td> \n",
    "     <td>\n",
    "         [[ 0.11017994  0.01105339]\n",
    "         [ 0.09466817  0.00949723]\n",
    "         [-0.05743092 -0.00576154]]\n",
    "      </td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "        <td>\n",
    "            dW\n",
    "        </td> \n",
    "        <td>\n",
    "            [[ 0.10266786  0.09778551 -0.01968084]]\n",
    "        </td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "        <td>\n",
    "        db\n",
    "       </td> \n",
    "       <td >\n",
    "           [[-0.05729622]]\n",
    "        </td> \n",
    "  </tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output with relu:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td > [[ 0.44090989  0.        ]\n",
    " [ 0.37883606  0.        ]\n",
    " [-0.2298228   0.        ]] </td> \n",
    "\n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.44513824  0.37371418 -0.10478989]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.20837892]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 - L-Model Backward \n",
    "\n",
    "이제 전체 네트워크에 해당하는 backward 함수를 구현합니다. \n",
    "\n",
    "\n",
    "`L_model_forward`함수를 구현하면서 각 계층은 cache에 (X,W,b, and z)를 저장해 둔것을 기억해야합니다. \n",
    "backward propagation에서는 이 cache에 저장된 값들을 사용합니다. $L$번 계층에서 시작하여 hidden layers들을 역방향으로 처리합니다. \n",
    "\n",
    "\n",
    "<img src=\"images/mn_backward.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center>  Figure 5 : Backward pass  </center></caption>\n",
    "\n",
    "backward propagation을 위해서는 가장 먼저 `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$를 계산해야합니다. \n",
    "이것은 다음처럼 하면 됩니다. 이미 구현해 두었으므로 확인만 하세요. \n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "```\n",
    "`dAL`을 구한후에는 \n",
    "LINEAR->SIGMOID backward함수에 `dAL`을 입력으로 주게 됩니다. \n",
    "\n",
    "그리고, for 루프에서 LINEAR->RELU 계층들에 대한 backward propagation을 진행합니다. 각 계층에서는 dA, dW, db를 `grads` 딕셔너리에 저장합니다. \n",
    "딕셔너리에 저장하기 위해서는 다음처럼 합니다:\n",
    "\n",
    "$grads[\"dW\" + str(l)] = dW^{[l]}$\n",
    "\n",
    "예를 들어, $l=3$일때는  `grads[\"dW3\"]`에 $dW^{[3]}$을 저장합니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)]= linear_activation_backward(dAL,caches[L-1],'sigmoid')\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        grads[\"dA\" + str(l)], grads[\"dW\" + str(l+1)], grads[\"db\" + str(l+1)]= linear_activation_backward(grads[\"dA\" + str(l+1)],caches[l],'relu')\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td > dW1 </td> \n",
    "           <td > [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n",
    " [ 0.          0.          0.          0.        ]\n",
    " [ 0.05283652  0.01005865  0.01777766  0.0135308 ]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > db1 </td> \n",
    "           <td > [[-0.22007063]\n",
    " [ 0.        ]\n",
    " [-0.02835349]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "  <td > dA1 </td> \n",
    "           <td > [[ 0.12913162 -0.44014127]\n",
    " [-0.14175655  0.48317296]\n",
    " [ 0.01663708 -0.05670698]] </td> \n",
    "\n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 - Update Parameters\n",
    "\n",
    "모델의 모든 파라미터들에 대한 gradients를 구했으니, 이제 파라미터들의 값을 gradient descent를 이용해 업데이트합니다:\n",
    "\n",
    "- $ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$\n",
    "- $ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$\n",
    "\n",
    "여기서, $\\alpha$는 learning rate입니다. 업데이트된 파라미터는 `parameters` 딕셔너리에 저장합니다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**:\n",
    "모든 계층 $l (l=1,2,...,L$)의 파라미터 $W^{[l]}$ and $b^{[l]}$에 대해서 gradient descent를 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                parameters[\"W\" + str(l)] = ... \n",
    "                parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    for l in range(1, L+1):\n",
    "        parameters[\"W\" + str(l)] =  parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] =  parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:100%\"> \n",
    "    <tr>\n",
    "    <td >W1</td> \n",
    "           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
    " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
    " [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "    <td > b1 </td> \n",
    "           <td > [[-0.04659241]\n",
    " [-1.28888275]\n",
    " [ 0.53405496]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > W2 </td> \n",
    "           <td > [[-0.55569196  0.0354055   1.32964895]]</td> \n",
    "  </tr> \n",
    "    <tr>\n",
    "    <td > b2 </td> \n",
    "           <td > [[-0.84610769]] </td> \n",
    "  </tr> \n",
    "</table>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7 - Conclusion\n",
    "\n",
    "\n",
    "이제 딥 뉴럴네트워크를 위한 모든 함수를 구현하였습니다. \n",
    "지금 구현한 딥 뉴럴네트워크를 다음 과제에서 cat vs. non-cat과 같은 분류기를 훈련시키는데 적용할 것입니다."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
